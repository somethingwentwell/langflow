version: "3"

services:
  backend:
    build:
      context: ./
      dockerfile: ./dev.Dockerfile
      # docker buildx build --platform=linux/amd64 -f dev.Dockerfile -t warching/langflow_backend .
    env_file:
      - .env
    ports:
      - "7860:7860"
    volumes:
      - ./:/app
    command: 
      - bash 
      - -c 
      - | 
        pip install pgvector llama-cpp-python
        uvicorn --factory src.backend.langflow.main:create_app --host 0.0.0.0 --port 7860 --workers 5 --reload

  frontend:
    build:
      context: ./src/frontend
      dockerfile: ./dev.Dockerfile
      # cd src/frontend && docker buildx build --platform=linux/amd64 -f dev.Dockerfile -t warching/langflow_frontend .
      args:
        - BACKEND_URL=http://backend:7860
    environment:
      - VITE_PROXY_TARGET=http://backend:7860
    ports:
      - "80:3000"
    volumes:
      - ./src/frontend/public:/home/node/app/public
      - ./src/frontend/src:/home/node/app/src
      - ./src/frontend/package.json:/home/node/app/package.json
    restart: on-failure
